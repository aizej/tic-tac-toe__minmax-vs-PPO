{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Action\n",
    "from tkinter import E\n",
    "from turtle import shape\n",
    "from aiohttp import request\n",
    "import gym\n",
    "from gym import Env\n",
    "from gym.spaces import Discrete, Box, Dict, Tuple, MultiBinary, MultiDiscrete \n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import win32api, win32con, win32gui\n",
    "from PIL import ImageGrab\n",
    "import time\n",
    "import requests\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import mss\n",
    "from matplotlib import pyplot as plt\n",
    "import re\n",
    "import cv2\n",
    "from scipy.ndimage import convolve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def make_a_play(board, play):\n",
    "    board[play[0]][play[1]][play[2]] = 1\n",
    "    return board\n",
    "\n",
    "\n",
    "def get_reward(info, lenght_to_win):\n",
    "    reward = 0\n",
    "    for i in info:\n",
    "        reward += (i-1)\n",
    "    reward = (reward**(lenght_to_win))/((lenght_to_win)**(lenght_to_win))\n",
    "    return reward\n",
    "\n",
    "\n",
    "def make_horizontal_win_board(lenght_to_win):\n",
    "    reward_board = [[]]\n",
    "    for x in range(lenght_to_win):\n",
    "        reward_board[0].append(1)\n",
    "    return reward_board\n",
    "\n",
    "def make_vertical_win_board(lenght_to_win):\n",
    "    reward_board = []\n",
    "    for y in range(lenght_to_win):\n",
    "        reward_board.append([1])\n",
    "    return reward_board\n",
    "\n",
    "def make_left_up_diagonal_win_board(lenght_to_win):\n",
    "    reward_board = []\n",
    "    for y in range(lenght_to_win):\n",
    "        reward_board.append([])\n",
    "        for x in range(lenght_to_win):\n",
    "            if y==x:\n",
    "                reward_board[y].append(1)\n",
    "            else:\n",
    "                reward_board[y].append(0)\n",
    "    return reward_board\n",
    "\n",
    "def make_right_up_diagonal_win_board(lenght_to_win):\n",
    "    reward_board = make_left_up_diagonal_win_board(lenght_to_win)\n",
    "    reward_board = np.flipud(reward_board)\n",
    "    return reward_board\n",
    "\n",
    "\n",
    "def check_for_win(board,lenght_to_win):\n",
    "    win_conditions = [make_vertical_win_board(lenght_to_win),make_horizontal_win_board(lenght_to_win),make_left_up_diagonal_win_board(lenght_to_win),make_right_up_diagonal_win_board(lenght_to_win)]\n",
    "    for win_board in win_conditions:\n",
    "        is_win = convolve(board,win_board,mode=\"constant\")\n",
    "        for y in is_win:\n",
    "            for x in y:\n",
    "                if x == lenght_to_win:\n",
    "                    return True\n",
    "        \n",
    "    return False\n",
    "\n",
    "def move_is_legal(playing_boards, play):\n",
    "    if playing_boards[0][play[1]][play[2]] == 0 and playing_boards[1][play[1]][play[2]] == 0:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def is_draw(playing_boards,board_size):\n",
    "    if np.sum(playing_boards) == board_size**2:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def flat_to_poss_in2d_array(n,size,player):\n",
    "    y = int((n)/size)\n",
    "    x = (n)- y*size\n",
    "    action = [player,y,x]\n",
    "    return action\n",
    "\n",
    "\n",
    "    \n",
    "def all_posible_moves(player,boards):\n",
    "    posible_moves = []\n",
    "    for y_index, y in enumerate(boards[0]):\n",
    "        for x_index, x in enumerate(y):\n",
    "            if boards[0][y_index][x_index] == 0 and boards[1][y_index][x_index] == 0:\n",
    "                posible_moves.append([player,y_index,x_index])\n",
    "    return posible_moves\n",
    "\n",
    "\n",
    "\n",
    "def minmax(boards, depth = 0):\n",
    "    best_move = None\n",
    "    score_of_best_move = 0\n",
    "    for posible_move in all_posible_moves(boards):\n",
    "        boards[posible_move[0]][posible_move[1]][posible_move[2]] = 1\n",
    "        player_win = check_for_win(boards[0],len(boards[0]))\n",
    "        oponent_win = check_for_win(boards[0],len(boards[0]))\n",
    "        draw = is_draw(boards[0],len(boards[0]))\n",
    "        if best_move\n",
    "\n",
    "        boards[posible_move[0]][posible_move[1]][posible_move[2]] = 0\n",
    "        \n",
    "        \n",
    "    \n",
    "    depth += 1\n",
    "    return minmax()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tim = []\n",
    "class pyz_driver(Env):\n",
    "    def __init__(self):\n",
    "        self.first_to_play = True\n",
    "        self.play_against_random = 0\n",
    "        \n",
    "        #setup the game\n",
    "        self.board_size = 4\n",
    "        self.lenght_to_win = 4\n",
    "        self.playing_boards = np.array([np.zeros((self.board_size,self.board_size)), np.zeros((self.board_size,self.board_size))])\n",
    "        self.playing_boards = self.playing_boards.astype(int)\n",
    "\n",
    "        self.action_space = Box(-1, 1, shape=(self.board_size**2, ),dtype=np.float32)\n",
    "\n",
    "        self.observation_space = Box(0,1, shape=(2,self.board_size,self.board_size),dtype=np.uint8)\n",
    "\n",
    "        self.state = self.playing_boards\n",
    "        self.boards = [[],[]]\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    def do_action(self,action):   \n",
    "        self.playing_boards[action[0]][action[1]][action[2]] = 1\n",
    "\n",
    "    def step(self, action):\n",
    "        reward = 0\n",
    "        start = time.time()\n",
    "        \n",
    "\n",
    "\n",
    "        action = action.argmax()\n",
    "        action = flat_to_poss_in2d_array(action,self.board_size,player=0)\n",
    "        if move_is_legal(self.playing_boards,action):\n",
    "            self.do_action(action)\n",
    "            #reward += 1\n",
    "        else:\n",
    "            reward -= 1\n",
    "            pass\n",
    "        \n",
    "\n",
    "        \n",
    "        player_win = check_for_win(self.playing_boards[0],self.lenght_to_win)\n",
    "        oponent_win = check_for_win(self.playing_boards[1],self.lenght_to_win)\n",
    "        draw = is_draw(self.playing_boards,self.board_size)\n",
    "\n",
    "        if player_win or oponent_win or draw:\n",
    "            done = True\n",
    "        else:\n",
    "            done = False\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        if not done:\n",
    "            if self.play_against_random <2:\n",
    "                posible_moves = all_posible_moves(player=1,boards=self.playing_boards)\n",
    "                oponent_action = random.choice(posible_moves)\n",
    "                self.do_action(oponent_action)\n",
    "            else:\n",
    "                rewersed_boards = np.array([self.playing_boards[1],self.playing_boards[0]])\n",
    "                oponent_action = model.predict(rewersed_boards)\n",
    "                oponent_action = oponent_action[0] # obasuhe tuple s vÃ­sledkem a NONE\n",
    "                oponent_action = oponent_action.argmax()\n",
    "                oponent_action = flat_to_poss_in2d_array(oponent_action,self.board_size,player=1)\n",
    "                if move_is_legal(self.playing_boards,oponent_action):\n",
    "                    self.do_action(oponent_action)\n",
    "            \n",
    "\n",
    "            player_win = check_for_win(self.playing_boards[0],self.lenght_to_win)\n",
    "            oponent_win = check_for_win(self.playing_boards[1],self.lenght_to_win)\n",
    "            draw = is_draw(self.playing_boards,self.board_size)\n",
    "\n",
    "            if player_win or oponent_win or draw:\n",
    "                done = True\n",
    "            else:\n",
    "                done = False\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        if player_win:\n",
    "            reward += 1#self.board_size**2\n",
    "            self.boards[0].append(self.playing_boards)\n",
    "            pass\n",
    "\n",
    "        if oponent_win:\n",
    "            reward -= 1\n",
    "            self.boards[1].append(self.playing_boards)\n",
    "            pass\n",
    "\n",
    "        if draw and not(oponent_win or player_win):\n",
    "            reward += 0.75\n",
    "            pass\n",
    "\n",
    "        if draw:\n",
    "            #reward += 10\n",
    "            pass\n",
    "\n",
    "        \n",
    "        tim.append(time.time()-start)\n",
    "        info = {}\n",
    "        return self.playing_boards , reward, done, info\n",
    "\n",
    "\n",
    "    def render(self):\n",
    "        print(self.playing_boards)\n",
    "\n",
    "    def reset(self):\n",
    "        self.playing_boards = np.array([np.zeros((self.board_size,self.board_size)), np.zeros((self.board_size,self.board_size))])\n",
    "        self.playing_boards = self.playing_boards.astype(int)\n",
    "        self.first_to_play = not self.first_to_play\n",
    "        \n",
    "        self.play_against_random += 1\n",
    "        if self.play_against_random == 4:\n",
    "            self.play_against_random = 0\n",
    "        \n",
    "        \n",
    "        if not self.first_to_play:\n",
    "            if self.play_against_random <2:\n",
    "                posible_moves = all_posible_moves(player=1,boards=self.playing_boards)\n",
    "                oponent_action = random.choice(posible_moves)\n",
    "                self.do_action(oponent_action)\n",
    "            else:\n",
    "                rewersed_boards = np.array([self.playing_boards[1],self.playing_boards[0]])\n",
    "                oponent_action = model.predict(rewersed_boards)\n",
    "                oponent_action = oponent_action[0] # obasuhe tuple s vÃ­sledkem a NONE\n",
    "                oponent_action = oponent_action.argmax()\n",
    "                oponent_action = flat_to_poss_in2d_array(oponent_action,self.board_size,player=1)\n",
    "                if move_is_legal(self.playing_boards,oponent_action):\n",
    "                    self.do_action(oponent_action)\n",
    "        \n",
    "        \n",
    "        \n",
    "        return self.playing_boards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "class TrainAndLoggingCallback(BaseCallback):\n",
    "\n",
    "    def __init__(self, check_freq, save_path, verbose=1):\n",
    "        super(TrainAndLoggingCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.save_path = save_path\n",
    "\n",
    "    def _init_callback(self):\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self):\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "            model_path = os.path.join(self.save_path, 'best_model_{}'.format(self.n_calls))\n",
    "            self.model.save(model_path)\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = pyz_driver()\n",
    "dir_id = time.time()\n",
    "learning_rate = 0.00005\n",
    "timesteps = 10_000_000\n",
    "info = f\"random_w_m_{env.board_size}x{env.board_size}_{env.lenght_to_win}_to_win_lr={learning_rate}\"\n",
    "\n",
    "CHECKPOINT_DIR = f'./train/{timesteps}_timesteps_{info}_{dir_id}'\n",
    "LOG_DIR = f'./logs/{timesteps}_timesteps_{info}_{dir_id}'\n",
    "\n",
    "callback = TrainAndLoggingCallback(check_freq=10000, save_path=CHECKPOINT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model = PPO(\"MlpPolicy\", env, tensorboard_log=LOG_DIR, verbose=1, learning_rate=learning_rate)\n",
    "#check_env(env)\n",
    "#model = PPO.load(r\"train\\2000000_timesteps_random_w_m_3x3_3_to_win_lr=5e-05_1681116393.3547344\\best_model_2000000.zip\", env=env)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.learn(total_timesteps=(timesteps), callback=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum(tim)/len(tim))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "72f235a78b5cf937fd09c1593b6a0e4473f824a03930b62c2c7d9a177b9de8f3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
